from nltk.tokenize import TweetTokenizer
from  file_manager import load_tracker
from corpus import Corpus

tknzr = TweetTokenizer()
original_corpus = "/Users/samuelrodriguezmedina/Google Drive/Language Technology/Research and Development/project/corpora/test_delete/"
final_corpus = "/Users/samuelrodriguezmedina/Documents/classes/research_and_development/uu-twitter-paraphrase-normaliser/"
c = Corpus()
c.create_corpus(original_corpus, final_corpus, 'es')

id_list = [
    "1055868943091597314",
    "1055869021969629184",
    "1055869027283812352",
    "1055869385800384513",
    "1055866409308356609",
    "1055866434365128705",
    "1055866574341595142",
    "1055866656839385096",
    "1058780630387953665",
    "1058780716744523776",
    "1058780724260691969",
    "1058780828476624901",
    "1058780951738748928",
    "1058668426640740358",
    "1058668482609537025",
    "1058668740101910529",
    "1058669855921131520",
    "1058671100631175168",
    "1058671137167675392",
    "1058671689394020353",
    "1058672714628694016",
    "1058674706059067392",
    "1058675020266913792",
    "1058677163313057792",
    "1058685351047389184",
    "1058685926325583872",
    "1058687464011952129",
    "1058688586873221120",
    "1058783471630475265",
    "1058784040839512066",
    "1058784066605080576",
    "1058784999414157312",
    "1058785403703083008",
    "1058787438653517824",
    "1058781446951854080",
    "1058781482087563265",
    "1058781649276674049",
    "1058781791220363264",
    "1058782060964466690",
    "1058782162072285184",
    "1058782216367546369",
    "1058782978367725568",
    "1058819284607090688",
    "1058826406535278593",
    "1058828215026900994",
    "1058832349616582656",
    "1058689147727241221",
    "1058689738771697664",
    "1058800463485849601",
    "1058801852639338496",
    "1058801902723522560",
    "1058807760148668417",
    "1058809734936096768",
    "1058716569356943360",
    "1058765062486310914",
    "1055863362058362880",
    "1058666686851833861",
    "1058667381407600640",
    "1058667707867021313",
    "1058659733698158593",
    "1058659765046439936",
    "1058659881106972672",
    "1058788883406749699",
    "1058789975158853633",
    "1058793497988399111",
    "1058794003288768512",
    "1058794813204676608",
    "1058795954416050176",
    "1058781144542601216",
    "1058781178927505409",
    "1058781271353184256",
    "1058654664432979970",
    "1058654785891692545",
    "1058654798982115328",
    "1058655524974223361",
    "1058656194615824385",
    "1058656393853591552",
    "1058656472878452736",
    "1058656616902520833",
    "1058656758015647745",
    "1058656983698628609",
    "1058657310954962945",
    "1058657613901168640",
    "1058657658658586624",
    "1058657738358747136",
    "1058658026901536768",
    "1058658308930842624",
    "1058658399934660609",
    "1058658648099033088",
    "1058658704768282624",
    "1058658790969692160",
    "1058659157644058624",
    "1058659541376790528",
    "1058659558862790656"
  ]
tracker = load_tracker(final_corpus, "cleaning", "inv_tracker.json")
list_tweets = c.get_tweets_by_ids(id_list, final_corpus, tracker)

for tw in list_tweets:
    print("=======")
    print(tw)
    print(tknzr.tokenize(tw))
